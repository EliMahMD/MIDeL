{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning and GraNd (Gradient Normed) Sampling\n",
    "\n",
    "*Authors: Sahika Betul Yayli, MD*\n",
    "\n",
    "## Introduction to Active Learning\n",
    "\n",
    "Active Learning is a powerful technique in machine learning that helps improve model performance while reducing the need for excessive manual labeling. Instead of training a model on a fully labeled dataset, Active Learning allows the model to decide which data points should be labeled, focusing on the most informative samples.\n",
    "\n",
    "This approach makes the labeling process more efficient by reducing the number of labels needed while still maintaining (or even improving) performance. Itâ€™s particularly useful in cases where labeling data is expensive or time-consuming, such as medical imaging, speech recognition, and other domains requiring expert annotation.\n",
    "\n",
    "## The GraNd (Gradient Normed) Method\n",
    "\n",
    "GraNd is a sampling technique used in active learning. It assigns a relevance score to each sample based on the average gradient of the loss. The method typically uses the gradients of the parameters in the penultimate layer of the network (just before the linear layer that yields the class probabilities).\n",
    "\n",
    "The penultimate layer is chosen for several reasons:\n",
    "1. It represents high-level features learned by the model.\n",
    "2. Gradients at this layer are more informative about the model's learning process than the final layer.\n",
    "3. The final layer often includes a softmax activation, which can mask the true uncertainty of the model.\n",
    "4. Using the penultimate layer makes the method more generalizable across different model architectures.\n",
    "\n",
    "Let's implement the GraNd method using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example model definition (e.g., a simple CNN)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(32 * 26 * 26, 10)  # Example dimensions\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Model and optimizer setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Sample data\n",
    "batch_size = 64\n",
    "data = torch.randn(batch_size, 1, 28, 28).to(device)  # Dummy MNIST-like data\n",
    "targets = torch.randint(0, 10, (batch_size,)).to(device)\n",
    "\n",
    "# GraNd score computation function\n",
    "def compute_grand_scores(model, data, targets):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(data.size(0)):\n",
    "        sample = data[i].unsqueeze(0)  # Process as individual sample\n",
    "        target = targets[i].unsqueeze(0)\n",
    "        \n",
    "        # Forward pass and loss computation\n",
    "        prediction = model(sample)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        \n",
    "        # Compute gradients (only for the penultimate layer)\n",
    "        grads = torch.autograd.grad(loss, model.fc1.weight, retain_graph=True)[0]\n",
    "        \n",
    "        # Calculate gradient norm (e.g., L2 norm)\n",
    "        grad_norm = grads.norm(2).item()\n",
    "        scores.append(grad_norm)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Compute GraNd scores\n",
    "grand_scores = compute_grand_scores(model, data, targets)\n",
    "\n",
    "# Sort to select samples that could provide the most information\n",
    "top_k_indices = sorted(range(len(grand_scores)), key=lambda i: grand_scores[i], reverse=True)[:10]\n",
    "print(\"Indices of the most informative samples:\", top_k_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the Code\n",
    "\n",
    "1. We define a simple CNN model with a convolutional layer and a fully connected layer.\n",
    "2. The `compute_grand_scores` function calculates the GraNd score for each sample:\n",
    "   - It computes the loss for each individual sample.\n",
    "   - It calculates the gradients with respect to the weights of the penultimate layer (fc1).\n",
    "   - It computes the L2 norm of these gradients as the GraNd score.\n",
    "3. We then sort the samples based on their GraNd scores and select the top k most informative samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying GraNd Sampling to nnU-Net\n",
    "\n",
    "Below is a basic implementation of GraNd for nnU-Net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "\n",
    "# Load the nnUNet model\n",
    "predictor = nnUNetPredictor(\n",
    "    tile_step_size=0.5,\n",
    "    use_gaussian=True,\n",
    "    use_mirroring=True,\n",
    "    perform_everything_on_device=True,\n",
    "    device=torch.device('cuda', 0),\n",
    "    verbose=False,\n",
    "    verbose_preprocessing=False,\n",
    "    allow_tqdm=True\n",
    ")\n",
    "\n",
    "predictor.initialize_from_trained_model_folder(\n",
    "    'path/to/your/model/folder',\n",
    "    use_folds=(0,),\n",
    "    checkpoint_name='checkpoint_final.pth'\n",
    ")\n",
    "\n",
    "# Access the penultimate layer\n",
    "penultimate_layer = predictor.network.seg_outputs[-1]\n",
    "\n",
    "# GraNd score computation function\n",
    "def compute_grand_scores(model, data, targets):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(data.size(0)):\n",
    "        sample = data[i].unsqueeze(0)\n",
    "        target = targets[i].unsqueeze(0)\n",
    "        \n",
    "        prediction = model(sample)\n",
    "        loss = torch.nn.functional.cross_entropy(prediction, target)\n",
    "        \n",
    "        grads = torch.autograd.grad(loss, penultimate_layer.weight, retain_graph=True)[0]\n",
    "        \n",
    "        grad_norm = grads.norm(2).item()\n",
    "        scores.append(grad_norm)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Load your data here\n",
    "# data = ...\n",
    "# targets = ...\n",
    "\n",
    "# Compute GraNd scores\n",
    "grand_scores = compute_grand_scores(predictor.network, data, targets)\n",
    "\n",
    "# Select top k samples\n",
    "top_k_indices = sorted(range(len(grand_scores)), key=lambda i: grand_scores[i], reverse=True)[:10]\n",
    "print(\"Indices of the most informative samples:\", top_k_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The GraNd method provides a way to select the most informative samples for active learning. By focusing on the gradients of the penultimate layer, it captures the model's uncertainty and learning potential for each sample. This approach can significantly reduce the amount of labeled data needed for training while maintaining or even improving model performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
