{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mayo-Radiology-Informatics-Lab/MIDeL/blob/main/chapters/13C.ipynb)"
      ],
      "metadata": {
        "id": "yDL-q_iPc_q3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Author: Yashbir Singh, PhD*"
      ],
      "metadata": {
        "id": "kjqzB3JeWxLZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k72N4ufXIbot"
      },
      "source": [
        "#Decision Tree Classification Algorithm#\n",
        "Decision Tree Classification is a popular machine learning algorithm that has shown promising results in medical imaging. It has been used to classify different medical images such as MRI, CT, and X-rays. In medical imaging, decision trees can be used for tasks such as classification, segmentation, and diagnosis.\n",
        "\n",
        "One of the benefits of using decision trees in medical imaging is the ability to identify relevant features or attributes that are important for classification. These features can include shape, texture, and intensity. For example, decision trees have been used to differentiate between benign and malignant tumors in breast cancer screening based on the shape and texture of the tumor.\n",
        "\n",
        "Another advantage of using decision trees is the ability to visualize the decision-making process. This can be useful in explaining the reasoning behind the model's prediction and in identifying errors or biases in the algorithm.\n",
        "\n",
        "However, decision trees may suffer from limitations such as overfitting or underfitting, which can result in poor performance on new data. To overcome these limitations, techniques such as pruning, regularization, and ensemble learning can be used.\n",
        "\n",
        "Overall, decision tree classification shows great potential in medical imaging for accurate and efficient diagnosis, which can lead to better patient outcomes.\n",
        "#Decision tree classification components:\n",
        "\n",
        "**Root node:** The topmost node of the tree, representing the entire dataset.\n",
        "\n",
        "**Internal node:** Intermediate nodes of the tree that represent the decision rules based on the features or attributes of the data.\n",
        "\n",
        "**Leaf node:** The terminal nodes of the tree that represent the final classification or decision.\n",
        "\n",
        "**Splitting criterion:** The algorithm uses a splitting criterion to determine which feature to use to split the data at each internal node. The most commonly used criteria are Gini impurity and Information gain.\n",
        "\n",
        "**Branch:** Each branch represents a possible outcome of the splitting criterion at the internal node.\n",
        "\n",
        "**Pruning:** A technique used to remove some of the branches of the decision tree to prevent overfitting and improve generalization performance.\n",
        "\n",
        "**Ensemble learning:** A technique that combines multiple decision trees to improve classification accuracy and reduce the risk of overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjEUp8hPIy3u"
      },
      "source": [
        "#Decision trees: Why use them?\n",
        "**Interpretability:** Decision trees are easy to interpret and visualize, making them useful for understanding the decision-making process behind a classification model. The nodes and branches of a decision tree represent the key features and their relationships in the data, making it easier to understand the model's decision-making process.\n",
        "\n",
        "**Non-parametric:**Decision trees are non-parametric, which means that they do not make any assumptions about the distribution of the data. This makes them useful for modeling complex relationships that may not be captured by more simple models such as linear regression.\n",
        "\n",
        "**Handling both continuous and categorical data:** Decision trees can handle both continuous and categorical data, making them versatile for a wide range of classification problems.\n",
        "\n",
        "**Robustness:** Decision trees are robust to outliers and missing data, which makes them useful for dealing with noisy data.\n",
        "\n",
        "**Speed:** Decision trees can be trained quickly, making them useful for large datasets with many features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H05qBH5JJ09h"
      },
      "source": [
        "#Steps of Decision Tree algorithm\n",
        "\n",
        "**Collect the training data:** This is the dataset that the Decision Tree will be trained on. The training data should consist of a set of input features (also known as predictors or independent variables) and corresponding output values (also known as response or dependent variable).\n",
        "\n",
        "**Choose the attribute for the root node:** The algorithm chooses an attribute that can split the dataset into two or more subsets that are as homogenous as possible in terms of the class labels. This process is repeated recursively for each subset until a stopping criterion is met.\n",
        "\n",
        "**Split the dataset:** The chosen attribute is used to split the dataset into two or more subsets. Each subset corresponds to a unique value of the chosen attribute.\n",
        "\n",
        "**Calculate information gain:** Information gain is a measure of the difference in impurity (or entropy) between the original dataset and the subsets created by splitting the dataset based on the chosen attribute. The attribute with the highest information gain is chosen as the next node in the tree.\n",
        "\n",
        "**Repeat the process:** The above steps are repeated recursively until all nodes in the tree have been created, or a stopping criterion is met. The stopping criterion could be a minimum number of instances per leaf, a maximum depth of the tree, or other rules.\n",
        "\n",
        "**Prune the tree (optional):** After the tree has been constructed, it is often pruned to reduce overfitting and improve the generalization performance of the model.\n",
        "\n",
        "**Apply the Decision Tree to new data:** Once the Decision Tree has been trained, it can be used to classify new data by traversing the tree from the root node to a leaf node based on the values of the input features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X0iS-c0KGgo"
      },
      "source": [
        "#Implementation of decision trees in Python Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_ABjh01KOnV"
      },
      "source": [
        "#Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXISvm5XHdbg",
        "outputId": "981b1976-d5e9-405d-a3f0-ba0f76a2247c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/smazzanti/mrmr\n",
            "  Cloning https://github.com/smazzanti/mrmr to /tmp/pip-req-build-vu0k5c2d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/smazzanti/mrmr /tmp/pip-req-build-vu0k5c2d\n",
            "  Resolved https://github.com/smazzanti/mrmr to commit 7d833dfa88fc800c0f65167c7e4d61c1b0073942\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting skfeature-chappers\n",
            "  Downloading skfeature_chappers-1.1.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from skfeature-chappers) (1.4.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from skfeature-chappers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from skfeature-chappers) (1.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->skfeature-chappers) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->skfeature-chappers) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->skfeature-chappers) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->skfeature-chappers) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->skfeature-chappers) (1.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->skfeature-chappers) (1.15.0)\n",
            "Installing collected packages: skfeature-chappers\n",
            "Successfully installed skfeature-chappers-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/smazzanti/mrmr\n",
        "!pip install skfeature-chappers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH6gOximHWtb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "from scipy import interp\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from skfeature.function.information_theoretical_based import MRMR\n",
        "\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a_dCTnTvCgv"
      },
      "source": [
        "To start with any project, we first have to get access to our data. We have prepared a set of data in the cloud, and you can download it using the 'gdown' command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTczK9nVy9nB",
        "outputId": "c1b54f79-19f0-4cc0-ba40-882edbd9de76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "replace Chapter_15/Chapter15/X.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gdown\n",
        "\n",
        "if not os.path.isdir(\"Chapter15\"):\n",
        "    gdown.download(\n",
        "        \"https://drive.google.com/uc?export=download&confirm=pbef&id=1cnn3ouGbLtmYJBXFLmDrzVWTSpK0f4VA\",\n",
        "        \"Chapter15.zip\",\n",
        "        quiet=True,\n",
        "    )\n",
        "    !unzip -q Chapter15.zip\n",
        "    os.remove(\"Chapter15.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYhRa1oZHkiv"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('X.csv')\n",
        "X = df.drop(['CoulmmnX'], axis=1)\n",
        "Y =  df['CoulmmnX']\n",
        "\n",
        "df_test = pd.read_csv('Y.csv')\n",
        "df_test = df_test.dropna()\n",
        "X_test = df_test.drop(['CoulmmnX'], axis=1)\n",
        "Y_test =  df_test['CoulmmnX']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYknxLMnK8vT"
      },
      "source": [
        "# To consider more sophisticated method of choosing which columns to keep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnllFJhLHpC0"
      },
      "outputs": [],
      "source": [
        "corr_matrix = X.corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.76)][1:]\n",
        "\n",
        "X_filtered = X.drop(X[to_drop], axis=1)\n",
        "X_test_filtered = X_test.drop(X_test[to_drop], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeQ2JlsGIy_F"
      },
      "outputs": [],
      "source": [
        "X_filtered = X_filtered.to_numpy()\n",
        "X_test_filtered = X_test_filtered.to_numpy()\n",
        "\n",
        "Y = Y.to_numpy()\n",
        "Y_test = Y_test.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLEQJ2fGLYXR"
      },
      "source": [
        "#Defining hyperparameters for models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNVdDncuLpNW"
      },
      "outputs": [],
      "source": [
        "classweight = 'balanced'\n",
        "maxdepth = None\n",
        "minsamplessplit = 2\n",
        "minsamplesleaf = 1\n",
        "maxleafnodes = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBEJ2lRQLsco"
      },
      "source": [
        "#Decision tree model and cross validation for each fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_HSIrSAH2lM"
      },
      "outputs": [],
      "source": [
        "kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=0)\n",
        "pred_test_full = 0\n",
        "cv_score = []\n",
        "pr_score = []\n",
        "re_score = []\n",
        "au_score = []\n",
        "i = 1\n",
        "\n",
        "for train_index, test_index in kf.split(X_filtered, Y):\n",
        "    xtr, xvl = X_filtered[train_index], X_filtered[test_index]\n",
        "    ytr, yvl = Y[train_index], Y[test_index]\n",
        "    lda = LDA(n_components = 1)\n",
        "    xtr_lda = lda.fit_transform(xtr, ytr)\n",
        "    xvl_lda = lda.transform(xvl)\n",
        "\n",
        "    model = tree.DecisionTreeClassifier(class_weight=classweight, max_depth=maxdepth, min_samples_split=minsamplessplit, min_samples_leaf=minsamplesleaf, max_leaf_nodes=maxleafnodes)\n",
        "\n",
        "    model.fit(xtr_lda, ytr)\n",
        "    score = metrics.accuracy_score(yvl, model.predict(xvl_lda))\n",
        "    score1 = metrics.precision_score(yvl, model.predict(xvl_lda))\n",
        "    score2 = metrics.recall_score(yvl, model.predict(xvl_lda))\n",
        "    score3 = metrics.roc_auc_score(yvl, model.predict(xvl_lda))\n",
        "    \n",
        "    print('Fold:', i)\n",
        "    print('Accuracy score:', score)\n",
        "    print('Precision score:', score1)\n",
        "    print('Recall score:', score2)\n",
        "    print('AUC score:', score3)\n",
        "    print('-------------')\n",
        "        \n",
        "    cv_score.append(score) \n",
        "    pr_score.append(score1)\n",
        "    re_score.append(score2)\n",
        "    au_score.append(score3)\n",
        "    \n",
        "    i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ytN0HOGIA8i"
      },
      "outputs": [],
      "source": [
        "print('\\nMean Accuracy', np.mean(cv_score))\n",
        "print('\\nMean Precision', np.mean(pr_score))\n",
        "print('\\nMean Recall', np.mean(re_score))\n",
        "print('\\nMean Auc', np.mean(au_score))\n",
        "\n",
        "print('\\nMedian Accuracy', np.median(cv_score))\n",
        "print('\\nMedian Precision', np.median(pr_score))\n",
        "print('\\nMedian Recall', np.median(re_score))\n",
        "print('\\nMedian Auc', np.median(au_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x336CljuIxn1"
      },
      "outputs": [],
      "source": [
        "lda = LDA(n_components = 1)\n",
        "X_filtered_lda = lda.fit_transform(X_filtered, Y)\n",
        "X_test_filtered_lda = lda.transform(X_test_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dj-WT-BIf6G"
      },
      "outputs": [],
      "source": [
        "model = tree.DecisionTreeClassifier(class_weight=classweight, max_depth=maxdepth, min_samples_split=minsamplessplit, min_samples_leaf=minsamplesleaf, max_leaf_nodes=maxleafnodes)\n",
        "\n",
        "model.fit(X_filtered_lda, Y)\n",
        "score = metrics.accuracy_score(Y_test, model.predict(X_test_filtered_lda))\n",
        "score1 = metrics.precision_score(Y_test, model.predict(X_test_filtered_lda))\n",
        "score2 = metrics.recall_score(Y_test, model.predict(X_test_filtered_lda))\n",
        "score3 = metrics.roc_auc_score(Y_test, model.predict(X_test_filtered_lda))\n",
        "print('Accuracy score:', score)\n",
        "print('Precision score:', score1)\n",
        "print('Recall score:', score2)\n",
        "print('AUC score:', score3)\n",
        "\n",
        "print('Test Set Confusion Matrix:')\n",
        "print(confusion_matrix(Y_test, model.predict(X_test_filtered_lda)))\n",
        "print('Train Set Confusion Matrix:')\n",
        "print(confusion_matrix(Y, model.predict(X_filtered_lda)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd5FCrcpL8gJ"
      },
      "source": [
        "#ROC plot for test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y2y_1qyJIK7"
      },
      "outputs": [],
      "source": [
        "y_pred_proba = model.predict(X_test_filtered_lda)\n",
        "\n",
        "fpr, tpr, _ = metrics.roc_curve(Y_test, y_pred_proba)\n",
        "auc = metrics.roc_auc_score(Y_test, y_pred_proba)\n",
        "plt.plot(fpr, tpr, label=\"ROC Score = \"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Radiomics + TDA')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.plot([0,1], [0,1], linestyle = '--', lw = 2, color = 'black')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}