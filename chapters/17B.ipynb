{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mayo-Radiology-Informatics-Lab/MIDeL/blob/main/chapters/17B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNRXnKgMErcy"
      },
      "source": [
        "<img src=\"https://github.com/Mayo-Radiology-Informatics-Lab/MIDeL/blob/main/chapters/img/logo.png?raw=true\" alt=\"HOPPR logo\n",
        "\" width=\"100%\">\n",
        "\n",
        "\n",
        "# Obtaining Consistent LLM Outputs: From Chaos to Clarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmFXBQtJErc0"
      },
      "source": [
        "### Introduction\n",
        "In this notebook, we will describe how to use the Instructor and Pydantic models to help increase the consistency of Large Language Model (LLM) output, which should increase the usefulness and accuracy.\n",
        "\n",
        "The main goal of this notebook is to provide you with a step-by-step guide on how to improve data consistency when working with LLMs. We will specifically focus on using the Pydantic and Instructor packages to validate JSON responses from an LLM.\n",
        "\n",
        "`instructor` is a lightweight Python library that provides a convenient wrapper around the client of the OpenAI compatible servers, adding validation of JSON responses from an LLM. Instructor uses the Pydantic library, which allows users to specify models for JSON schemas and data validation, ensuring that LLM responses adhere to the defined schema.\n",
        "\n",
        "\n",
        "#### Key Features\n",
        "- **Easy integration** Seamlessly integrates with several LLMs beyond OpenAI. See:\n",
        "    - Working with different providers: https://python.useinstructor.com/\n",
        "    - Examples: https://python.useinstructor.com/learning/\n",
        "- **Data validation**: Ensure the JSON response from a LLM meets the specified schema. See:\n",
        "    - https://docs.pydantic.dev/latest/\n",
        "- **Retry Management**: Retries with error guidance if the LLM returns invalid responses. You can set the maxium number of retries.\n",
        "- **Streaming Support**: Work with Lists and Partial responses effortlessly\n",
        "\n",
        "\n",
        "\n",
        "#### Concept\n",
        "<img src=\"https://github.com/Mayo-Radiology-Informatics-Lab/MIDeL/blob/main/chapters/img/17b.jpg?raw=true\" alt=\"Concept Image\" width=\"100%\">\n",
        "\n",
        "\n",
        "\n",
        "By using the Instructor package, you can have full control over agent flows without relying on complex agent frameworks. It serves as a starting point for building your own agents and ensures that the responses from LLMs are consistent and conform to the defined schema.\n",
        "\n",
        "In the next sections, we will walk through the steps involved in enhancing data consistency using Pydantic models and the Instructor package. We will cover topics such as port forwarding, installation, creating the client, defining the response model, prompting, and more.\n",
        "\n",
        "Let's dive in and explore the power of Pydantic models and the Instructor package in achieving data consistency in language model applications!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB8-Nl94Ew8v"
      },
      "source": [
        "## Step 0: Create a LLM server with ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run this notebook, we need to have a OpenAI Compatible server. You can connect you own OpenAI account, huggingface CLI or use a local server. In the next cell, we will create an LLM server running on colab so that you dont' need to use any of the prior options.\n",
        "> Note: If you are running this code on the Google Colab, be sure to check if you have a GPU (Runtime menu->`Change runtime type`->`gpu T4`)."
      ],
      "metadata": {
        "id": "lNMkul6Av_o0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfPJ3rM8mrs1",
        "outputId": "04972c41-630f-4fe7-a2e3-561e8d5c968a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "# Download and install Ollama which will serve the LLM\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can choose your desired model from https://ollama.com/library. In this notebook, we will use llama3.2 as an example."
      ],
      "metadata": {
        "id": "AwiwNP5WQoa8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UQuNiabfNlmW"
      },
      "outputs": [],
      "source": [
        "# Importing nesseracy libraries\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start ollama in the background and use llama3.1 model\n",
        "\n",
        "# Start the process in the background\n",
        "server = subprocess.Popen(['ollama', 'serve'])\n",
        "time.sleep(60) # To make sure ollama is ready in subsequent cell if you are running all not cell at a time\n",
        "\n",
        "# To kill the server\n",
        "# server.kill()\n",
        "\n",
        "# To see all the models available: https://ollama.com/library\n",
        "MODEL = 'llama3.2'\n",
        "llama3 = subprocess.Popen(['ollama', 'run', MODEL])\n",
        "time.sleep(90) # Make sure ollama is ready in subsequent cell if you are running all not cell at a time\n",
        "\n",
        "# To kill the llama3\n",
        "# llama3.kill()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rich\n",
        "# show which model(s) ollama is serving\n",
        "process = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
        "print(process.stdout)\n",
        "\n",
        "# Check if the MODEL is in the output\n",
        "if MODEL in process.stdout:\n",
        "    rich.print(f\"Model '{MODEL}' is successfully running.\")\n",
        "else:\n",
        "    rich.print(f\"Model '{MODEL}' is not currently running.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Q-idUacltj7g",
        "outputId": "af3b24f2-12d5-4a68-e829-99603bc09e85"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME               ID              SIZE      MODIFIED      \n",
            "llama3.2:latest    a80c4f17acd5    2.0 GB    2 minutes ago    \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Model \u001b[32m'llama3.2'\u001b[0m is successfully running.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Model <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2'</span> is successfully running.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation\n"
      ],
      "metadata": {
        "id": "OYkN1ni-g12r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To install the required packages, run the following command in your terminal:"
      ],
      "metadata": {
        "id": "9wIh2m-4wD9_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QbYMVbpmrs2",
        "outputId": "74b6e05e-edb2-4167-a987-b07cfacdc20e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting instructor\n",
            "  Downloading instructor-1.8.2-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (13.9.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from instructor) (3.11.15)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.11/dist-packages (from instructor) (0.16)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from instructor) (3.1.6)\n",
            "Collecting jiter<0.9,>=0.6.1 (from instructor)\n",
            "  Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from instructor) (1.78.1)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from instructor) (2.33.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from instructor) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10.0.0,>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from instructor) (9.1.2)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from instructor) (0.15.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich) (2.19.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor) (3.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.52.0->instructor) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.52.0->instructor) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.52.0->instructor) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.52.0->instructor) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->instructor) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->instructor) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->instructor) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->instructor) (2025.4.26)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.9.0->instructor) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.9.0->instructor) (1.5.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->instructor) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->instructor) (0.16.0)\n",
            "Downloading instructor-1.8.2-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.6/345.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, instructor\n",
            "  Attempting uninstall: jiter\n",
            "    Found existing installation: jiter 0.9.0\n",
            "    Uninstalling jiter-0.9.0:\n",
            "      Successfully uninstalled jiter-0.9.0\n",
            "Successfully installed instructor-1.8.2 jiter-0.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install instructor pydantic rich PyYAML tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p9Y0gfuVErc1"
      },
      "outputs": [],
      "source": [
        "# Importing the required libraries\n",
        "import json\n",
        "from pydantic import BaseModel, Field\n",
        "from pydantic.config import ConfigDict\n",
        "from typing import List, Literal, Optional, Any, Dict\n",
        "\n",
        "import instructor\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 1: Prompting A *Standard* Client\n",
        "\n",
        "Ready to see how a standard client handles things? As a benchmark, let's set up a regular OpenAI client and give it the task of extracting specific information from an interventional radiology report. We'll see how it performs before we introduce our enhanced approach!\n",
        "\n",
        "### Configurations\n",
        "In the following code block, we will set up our configuration settings.\n"
      ],
      "metadata": {
        "id": "ZFQMuNg8hs0R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EK1ARF-5Erc3"
      },
      "outputs": [],
      "source": [
        "# Configure the main settings for the LLM interaction.\n",
        "HOST = \"http://localhost:11434\" # The base URL for the LLM API endpoint. The default is for Ollama running locally.\n",
        "\n",
        "API_KEY = \"ollama\" # The API key for authentication with the LLM service. This is often required, but may be unused depending on the service (e.g., for local Ollama).\n",
        "\n",
        "MODEL = MODEL # The specific language model to be used for generating responses.\n",
        "\n",
        "SEED = 42 # A random seed for reproducibility of model outputs.\n",
        "\n",
        "TEMP = 0.0 # The temperature setting for controlling the randomness of the model's output.\n",
        "\n",
        "MAX_RETRIES = 5 # The maximum number of attempts the instructor library will make to validate the response from the LLM if the initial response is invalid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASTdytasmrs4"
      },
      "source": [
        "It is time to create the client. `client` is responsible to contact the LLM server we have and return the response. In this notebook we are using OpenAI compatible server/clients. In case you are using `ollama`, it won't change the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "daxCMSGAmrs4"
      },
      "outputs": [],
      "source": [
        "# Create the client\n",
        "standard_client = OpenAI(\n",
        "    base_url=f\"{HOST}/v1\",\n",
        "    api_key=API_KEY,  # required, but unused\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our client is configured, in the following code blocks, we will craft the prompts necessary to instruct the language model. We will define both a system prompt, which sets the overall context and role of the model, and a user prompt, which contains the specific task and guidelines for extraction. Following that, we will utilize a sample radiology report to test the model's ability to extract the desired information based on our defined prompts.\n"
      ],
      "metadata": {
        "id": "QMbnwtEwi3_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A sample radiology report for testing the model\n",
        "sample_interventional_report = \"\"\"\n",
        "EXAM:  MR PELVIS ABLATION, US ASSISTED GUIDANCE\n",
        "  PROCEDURE: Percutaneous cryoablation of left seminal vesicle for recurrent  prostate cancer\n",
        "  PRE-PROCEDURE: Patient seen and evaluated. Allergies, pertinent medications, and  history reviewed.\n",
        "  Discussed risks, benefits, alternatives for procedure, and  obtained informed consent.\n",
        "  Patient understands information and questions  answered. Immediately prior to starting the procedure, in the presence of the  assisting personnel,\n",
        "  procedural pause was conducted to verify correct patient  identity and verification of procedure to be performed, and as applicable,\n",
        "  correct side and site, correct patient position, availability of implants,  special equipment, or special requirements, and all image and specimen  identification data.\n",
        "  The roles and responsibilities of care team members,  residents, and fellows were discussed.\n",
        "  General anesthesia and/or sedation  provided by Anesthesiology.\n",
        "\n",
        "  TECHNIQUE: Using sterile technique, combined ultrasound and MRI guidance, and  general anesthesia provided by the department of anesthesiology,\n",
        "  percutaneous  cryoablation of left seminal vesicle was performed using 3 IceRod cryoneedles  for recurrent prostate cancer.\n",
        "\n",
        "  FINDINGS: Patient was brought into the MR anteroom where anesthesia was  initiated.\n",
        "  Patient was transferred to the MR table in a supine feet-first  position. Patient was then brought into the MR suite and the legs were  positioned in a semi-frogleg position.\n",
        "  Perineum was sterilely draped and  prepped. Transperineal guidance grid was placed against the perineum.\n",
        "  Cryoneedles were tested. Under direct US guidance, Abbocath cryoneedle guides  were placed and cryoneedles were placed into the recurrent prostate cancer.\n",
        "  Position was confirmed with MR imaging. Saline infusion needle placed. Freezing  was initiated. A total of three freeze-thaw cycles were performed.\n",
        "  During  freezing, iceball growth monitored with continuous MR imaging. After the final  freeze-thaw cycle, the needles were removed.  Dynamic contrast enhanced images  were performed.\n",
        "  These demonstrated good coverage of the prostate cancer. Patient  was brought out of the MR suite and awoke. Patient tolerated the procedure well  with no immediate complications.\n",
        "  Anesthesia: General Anesthesia  Imaging Guidance: Combined MR/US\n",
        "  Approach: Standard  Adjunctive Procedures: saline displacement of the rectum\n",
        "  Ablation Parameters: 3 Freeze-thaw cycles  Probe Removal: none\n",
        "  COMPLICATIONS: none\n",
        "  POST-PROCEDURE TECHNICAL EVALUATION: Zone of ablation encompassed tumor(s)  completely.\n",
        "  IMPRESSION:  Successful image-guided cryoablation of left seminal vesicle for  recurrent prostate cancer.\n",
        "  Follow-up imaging recommended specifically 6 month  followup with prostate MRI, PSA, clinic visit.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Inzjk4i3XCSv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "K2x5jtLOmrs4"
      },
      "outputs": [],
      "source": [
        "# Test prompts\n",
        "test1_system_prompt = \"You are an expert in extracting data from radiology reports with 20 years of experience.\"\n",
        "test1_user_prompt = \"\"\"\n",
        "Extract data elements from the MR guided ablation report in the <report> tag:\n",
        "\n",
        "    Guidelines:\n",
        "    - Focus on findings at the time of scan, not previous ones.\n",
        "    - If information is not mentioned, use 'Not Specified'.\n",
        "    - Ignore irrelevant information.\n",
        "    - Use only the provided output format.\n",
        "    - Expand abbreviations: sv (seminal vesicle), uvj (urethro vesicular junction), vuj (vesico urethral junction), VM (vascular malformation), US (ultrasound), LN (lymph node), CT (computed tomography), MRI (magnetic resonance imaging).\n",
        "\n",
        "    only return your answer in this json object. Include these keys in your response:\n",
        "    - organ: Extract the organ where the ablation was performed. Indicate 'Not Mentioned' if not specified. Use the provided dictionary to expand abbreviations.\n",
        "    - location: the exact anatomical location of the tissue\n",
        "    - tissueType: Specify the tissue type ablated: 'Muscle', 'Nerve', 'Fat', 'Ligament', 'Tendon', 'Cartilage', 'Bone', or 'Not Mentioned'. You can choose mmultuple tissues.\n",
        "    - complications: Specify whether complications occurred: 'Yes', 'No', or 'Not Mentioned'\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "id": "iUTXw321mrs5",
        "outputId": "ca9085b5-22c3-4dcd-f3fa-ba316a4bd3f1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output: ```\n",
              "\u001b[1m{\u001b[0m\n",
              "  \u001b[32m\"organ\"\u001b[0m: \u001b[32m\"Seminal Vesicle\"\u001b[0m,\n",
              "  \u001b[32m\"location\"\u001b[0m: \u001b[32m\"Left Seminal Vesicle\"\u001b[0m,\n",
              "  \u001b[32m\"tissueType\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m\"Muscle\"\u001b[0m, \u001b[32m\"Fat\"\u001b[0m\u001b[1m]\u001b[0m,\n",
              "  \u001b[32m\"complications\"\u001b[0m: \u001b[32m\"No\"\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n",
              "```\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Output: ```\n",
              "<span style=\"font-weight: bold\">{</span>\n",
              "  <span style=\"color: #008000; text-decoration-color: #008000\">\"organ\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Seminal Vesicle\"</span>,\n",
              "  <span style=\"color: #008000; text-decoration-color: #008000\">\"location\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Left Seminal Vesicle\"</span>,\n",
              "  <span style=\"color: #008000; text-decoration-color: #008000\">\"tissueType\"</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Muscle\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Fat\"</span><span style=\"font-weight: bold\">]</span>,\n",
              "  <span style=\"color: #008000; text-decoration-color: #008000\">\"complications\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"No\"</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "```\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Let's ask the model!\n",
        "\n",
        "# Creating the conversation for the model to pass report and instructions\n",
        "messages1 = [\n",
        "        {\"role\": \"system\", \"content\": test1_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"{test1_user_prompt} \\n <report> {sample_interventional_report} </report>\"}\n",
        "    ]\n",
        "\n",
        "\n",
        "# Asking the model to extract the requested information\n",
        "resp1 = standard_client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=messages1,\n",
        "        temperature=TEMP,\n",
        "        seed=SEED,\n",
        ").choices[0].message.content\n",
        "\n",
        "\n",
        "rich.print('Output:', resp1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is the response a valid JSON? Let's check!"
      ],
      "metadata": {
        "id": "dIsd_PNOzdKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's validate our response\n",
        "\n",
        "try:\n",
        "    json.loads(resp1)\n",
        "    print(\"response is a valid JSON object.\")\n",
        "except json.JSONDecodeError:\n",
        "    print(\"response is NOT a valid JSON object.\")"
      ],
      "metadata": {
        "id": "DbY1um26aUK2",
        "outputId": "e816c222-a0a5-49de-f2b4-c08fd9086326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response is NOT a valid JSON object.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhbWZSiamrs5"
      },
      "source": [
        "We clearly asked the LLM to give us an answer in json format, but it didn't! And every time that you run you query there is no guarantee that you get the same response structure. Therefore, we need another tools to force LLM and make sure always get a similar response structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPP8AhNCErc2"
      },
      "source": [
        "---\n",
        "## Step 2: Prompting A *Patched* Client\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we are focusing on a simple question answering (QA) task. For additional use cases, refer to [the cookbooks](https://python.useinstructor.com/examples/). to access to various examples demonstrating how to use Instructor in different scenarios.\n",
        "\n",
        "As a simple test example, let's prompt the LLM to extract specific pieces of information from text we supply. We will then compare its answer with what we know to be the answer.\n",
        "In the next cell, we write a test `system prompt` (which sets the personality or backstory of our LLM instance) and a test `user prompt` (which is the main task or request we are making, including guidelines of how to create or format the answer).\n"
      ],
      "metadata": {
        "id": "8qxkeLRTw0YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ceate a patched client using instructor\n",
        "patched_client = instructor.from_openai(\n",
        "    OpenAI(\n",
        "        base_url=f\"{HOST}/v1\",\n",
        "        api_key=API_KEY,  # required, but unused\n",
        "    ),\n",
        "    # mode: for more information: https://python.useinstructor.com/concepts/patching/\n",
        "    mode=instructor.Mode.JSON,\n",
        ")"
      ],
      "metadata": {
        "id": "bJ9XMgLYCmT9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71wN_Wq6mrs5"
      },
      "source": [
        "#### Pydantic Models\n",
        "Pydantic models are classes that inherit from pydantic.BaseModel. They offer several key benefits:\n",
        "\n",
        "- **Data Validation**: Models automatically validate input data, ensuring that it conforms to the defined field types and constraints.\n",
        "- **Type Hinting**: Models leverage Python's type annotations, providing clear type information for fields.\n",
        "- **Serialization**: Models can easily convert to and from JSON, making them ideal for API development.\n",
        "- **Schema Generation**: Pydantic can automatically generate JSON schemas from models, useful for documentation and API specifications.\n",
        "\n",
        "\n",
        "To create a Pydantic model, simply define a class that inherits from `BaseModel`. In the next code block, fields can be customized using the `Field` function. We are also using `typing` package. With the combination of these two packages, we can force the LLM to only response in the desired format:\n",
        "- `str`: Free from response. There is no limitation for the model. Although we can use max_length to limit the field.\n",
        "- `Literal`: Imagine that is similar to multiple choice question. LLM can only choose one of them.\n",
        "- `List`: LLM would return multiple objects in a list. We are using `List` in tandem with `Literal` to force LLM return in a specific terminology, like checking the checkboxes.\n",
        "\n",
        "    > **Note:** the term 'model' is used heavily in AI. When we refer to Pydantic 'model' we do not mean an AI or LLM model. Instead, it means a model of how the data should be represented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sKW1VIXdmrs5"
      },
      "outputs": [],
      "source": [
        "# Defining a simple \"ProstateModel\" response model to understand the pydantic models\n",
        "\n",
        "class ProstateModel(BaseModel):\n",
        "    # Each attribute has a description that will be used by the model to generate the response\n",
        "    organ: str = Field(...,\n",
        "        description=\"Extract the organ where the ablation was performed. Indicate 'Not Specified' if not specified. Use the provided dictionary to expand abbreviations.\"\n",
        "    )\n",
        "    location: str = Field(...,\n",
        "        description=\"Extract the specific anatomical location within the organ where the ablation was performed. Indicate 'Not Mentioned' if not specified. Use the provided dictionary to expand abbreviations.\"\n",
        "    )\n",
        "    tissueType: List[Literal['Muscle', 'Nerve', 'Fat', 'Ligament', 'Tendon', 'Cartilage', 'Bone', 'Not Specified']] = Field(...,\n",
        "        description=\"Specify the ablated tissue type. You can choose multiple tissues\"\n",
        "    )\n",
        "    complications: Literal['Yes', 'No', 'Not Specified'] = Field(...,\n",
        "        description=\"Specify whether complications occurred.\"\n",
        "    )\n",
        "\n",
        "    # OPTIONAL: We can include an example in the pydantic model. Therefore our LLM would have behave like a FewShot classification task.\n",
        "    model_config = ConfigDict(\n",
        "        json_schema_extra={\n",
        "        'examples':\n",
        "            [\n",
        "                {\n",
        "                    \"organ\": \"Liver\",\n",
        "                    \"location\": \"Dome\",\n",
        "                    \"tissueType\": [\"Bone\"],\n",
        "                    \"complications\": \"No\",\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GorLPPf5mrs6"
      },
      "source": [
        "Now that we have a test response model, let's ask the LLM again, but also give it this model for its reponse, so we also remove the response structure from the `user_prompt`. Note the 'response_model' parameter in the client.chat.completions.create function call. It was 'str' before, meaning it coul dbe any legal string value. By using TestModel as the response_model, the LLM will respond in a way that conforms to the TestModel. This time it should work!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "w9pvnBkumrs6"
      },
      "outputs": [],
      "source": [
        "# Test prompts\n",
        "test2_system_prompt = \"You are an expert in extracting data from radiology reports with 20 years of experience.\"\n",
        "test2_user_prompt = \"\"\"\n",
        "Extract data elements from the MR guided ablation report in the <report> tag:\n",
        "\n",
        "    Guidelines:\n",
        "    - Focus on findings at the time of scan, not previous ones.\n",
        "    - Ignore irrelevant information.\n",
        "    - Use only the provided output format.\n",
        "    - Expand abbreviations: sv (seminal vesicle), uvj (urethro vesicular junction), vuj (vesico urethral junction), VM (vascular malformation), US (ultrasound), LN (lymph node), CT (computed tomography), MRI (magnetic resonance imaging).\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "id": "SFCn-rpZmrs6",
        "outputId": "22fe554d-0ebd-40d1-b5e7-a248b077b961"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output: \u001b[1m{\u001b[0m\n",
              "    \u001b[32m\"organ\"\u001b[0m: \u001b[32m\"Seminal Vesicle\"\u001b[0m,\n",
              "    \u001b[32m\"location\"\u001b[0m: \u001b[32m\"Left Seminal Vesicle\"\u001b[0m,\n",
              "    \u001b[32m\"tissueType\"\u001b[0m: \u001b[1m[\u001b[0m\n",
              "        \u001b[32m\"Not Specified\"\u001b[0m\n",
              "    \u001b[1m]\u001b[0m,\n",
              "    \u001b[32m\"complications\"\u001b[0m: \u001b[32m\"No\"\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Output: <span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">\"organ\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Seminal Vesicle\"</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">\"location\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Left Seminal Vesicle\"</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">\"tissueType\"</span>: <span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">\"Not Specified\"</span>\n",
              "    <span style=\"font-weight: bold\">]</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">\"complications\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"No\"</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Let's ask the model\n",
        "\n",
        "# Creating the conversation for the model to pass report and instructions\n",
        "messages2 = [\n",
        "        {\"role\": \"system\", \"content\": test2_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"{test2_user_prompt} \\n <report> {sample_interventional_report} </report>\"}\n",
        "]\n",
        "\n",
        "# Asking the model to extract the requested information\n",
        "resp2 = patched_client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        response_model=ProstateModel,\n",
        "        messages=messages2,\n",
        "        temperature=TEMP,\n",
        "        seed=SEED,\n",
        "        max_retries=MAX_RETRIES,\n",
        ").model_dump_json(indent=4)\n",
        "\n",
        "\n",
        "rich.print('Output:', resp2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's validate our response\n",
        "\n",
        "try:\n",
        "    json.loads(resp2)\n",
        "    rich.print(\"response is a valid JSON object.\")\n",
        "except json.JSONDecodeError:\n",
        "    rich.print(\"response is NOT a valid JSON object.\")"
      ],
      "metadata": {
        "id": "256mAyJMbbeY",
        "outputId": "7293cdb9-1433-4c54-facc-c41b756b6df9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "response is a valid JSON object.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">response is a valid JSON object.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qklKcxm2mrs7"
      },
      "source": [
        "Do you see the differences? By providing a structure to the `response_model` and  `max_retries` parameters, we are able to extract the requested information from the text and present it in a structured format."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 3: A Complex Extraction\n",
        "\n"
      ],
      "metadata": {
        "id": "DT9W2k0wgWLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's tackle a more challenging extraction scenario often encountered in diagnostic radiology: analyzing a chest X-ray (CXR) report. These reports frequently contain intricate findings and subtle details that require a more sophisticated approach to capture accurately. To handle this complexity, we will leverage the power of nested Pydantic models. By structuring our validation models in a hierarchical way, we can represent the relationships between different findings, such as specific abnormalities within a particular lung zone or characteristics of a nodule. This allows us to extract a richer, more detailed, and highly structured representation of the information contained within the CXR report, moving beyond simple flat data extraction to a more nuanced understanding."
      ],
      "metadata": {
        "id": "Kr15BOpxveNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a tree structure using Pydantic models to represent the findings from a CXR report. This will involve nesting models to capture the hierarchical nature of the data.\n",
        "\n",
        "Here's the conceptual tree:\n",
        "```\n",
        "CXRExtraction\n",
        "├── airspace_opacity\n",
        "│   ├── consolidation\n",
        "│   │   └── modifiers\n",
        "│   └── ground_glass\n",
        "│       └── modifiers\n",
        "└── lung_nodules\n",
        "    ├── nodule\n",
        "    │   ├── distribution\n",
        "    │   └── modifiers\n",
        "    └── mass\n",
        "        ├── distribution\n",
        "        └── modifiers\n",
        "```"
      ],
      "metadata": {
        "id": "U8P4artkmybq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement the tree and create a pydantic model."
      ],
      "metadata": {
        "id": "gUuRj80MxF2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# AIRSPACE OPACITY COMPONENTS\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "class OpacityModifiers(BaseModel):\n",
        "    \"\"\"\n",
        "    Attributes that describe contextual characteristics of a radiographic finding.\n",
        "    \"\"\"\n",
        "    lung_lobe: Optional[Literal[\n",
        "        \"Right Upper\", \"Right Middle\", \"Right Lower\",\n",
        "        \"Left Upper\", \"Left Lower\", \"Not Specified\"\n",
        "    ]] = Field(\n",
        "        default=None,\n",
        "        description=\"Select the lung lobe where the finding is located.\"\n",
        "    )\n",
        "\n",
        "    chronicity: Optional[Literal[\n",
        "        \"Acute\", \"Subacute\", \"Chronic\", \"Not Specified\"\n",
        "    ]] = Field(\n",
        "        default=None,\n",
        "        description=\"Specify the time course of the finding.\"\n",
        "    )\n",
        "\n",
        "class Consolidation(BaseModel):\n",
        "    \"\"\"\n",
        "    Alveolar filling (e.g., pneumonia) visualized on chest imaging.\n",
        "    \"\"\"\n",
        "    present: bool = Field(...,\n",
        "        description=\"Indicate if consolidation is present.\"\n",
        "    )\n",
        "    extent: Optional[str] = Field(\n",
        "        default=None,\n",
        "        description=\"Describe the extent of consolidation (e.g., lobar, segmental).\"\n",
        "    )\n",
        "    modifiers: Optional[OpacityModifiers] = Field(\n",
        "        default=None,\n",
        "        description=\"Add location and chronicity details for the consolidation.\"\n",
        "    )\n",
        "\n",
        "class GroundGlass(BaseModel):\n",
        "    \"\"\"\n",
        "    Hazy increased lung opacity that does not obscure vessels.\n",
        "    \"\"\"\n",
        "    present: bool = Field(...,\n",
        "    description=\"Indicate if ground-glass opacity is present.\"\n",
        "    )\n",
        "    pattern: Optional[str] = Field(\n",
        "        default=None,\n",
        "        description=\"Describe the pattern of ground-glass opacity (e.g., reticular, crazy paving).\"\n",
        "    )\n",
        "    modifiers: Optional[OpacityModifiers] = Field(\n",
        "        default=None,\n",
        "        description=\"Add location and chronicity details for the ground-glass finding.\"\n",
        "    )\n",
        "\n",
        "class AirspaceOpacity(BaseModel):\n",
        "    \"\"\"\n",
        "    Combines airspace opacities like consolidation and ground-glass.\n",
        "    \"\"\"\n",
        "    consolidation: Optional[Consolidation] = Field(\n",
        "        default=None,\n",
        "        description=\"Describe findings related to consolidation.\"\n",
        "    )\n",
        "    ground_glass: Optional[GroundGlass] = Field(\n",
        "        default=None,\n",
        "        description=\"Describe findings related to ground-glass opacities.\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "Ps7OCWERlJRa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# LUNG NODULES AND MASSES\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "class NoduleModifiers(BaseModel):\n",
        "    \"\"\"\n",
        "    Attributes that describe contextual characteristics of a radiographic finding.\n",
        "    \"\"\"\n",
        "    lung_lobe: Optional[Literal[\n",
        "        \"Right Upper\", \"Right Middle\", \"Right Lower\",\n",
        "        \"Left Upper\", \"Left Lower\"\n",
        "    ]] = Field(\n",
        "        default=None,\n",
        "        description=\"Select the lung lobe where the finding is located.\"\n",
        "    )\n",
        "\n",
        "    calcification: Optional[Literal[\n",
        "        \"None\", \"Punctate\", \"Coarse\", \"Diffuse\"\n",
        "    ]] = Field(\n",
        "        default=None,\n",
        "        description=\"Describe the pattern of calcification within the finding.\"\n",
        "    )\n",
        "    size_mm: Optional[float] = Field(\n",
        "        default=None,\n",
        "        description=\"Enter the maximum diameter of the nodule in millimeters.\"\n",
        "    )\n",
        "    suspicious: Optional[bool] = Field(\n",
        "        default=None,\n",
        "        description=\"Indicate if the nodule is radiologically suspicious.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class Nodule(BaseModel):\n",
        "    \"\"\"\n",
        "    Small rounded opacity in the lung (<3 cm).\n",
        "    \"\"\"\n",
        "    distribution: Optional[Literal[\n",
        "        \"Solitary\", \"Multiple\", \"Diffuse\"\n",
        "    ]] = Field(\n",
        "        default=None,\n",
        "        description=\"Describe the spatial distribution of nodules.\"\n",
        "    )\n",
        "    modifiers: Optional[NoduleModifiers] = Field(\n",
        "        default=None,\n",
        "        description=\"Add location and chronicity details for the nodule.\"\n",
        "    )\n",
        "\n",
        "class Mass(BaseModel):\n",
        "    \"\"\"\n",
        "    Larger lung lesion (>3 cm) that may represent malignancy or organized pathology.\n",
        "    \"\"\"\n",
        "    distribution: Optional[Literal[\n",
        "        \"Solitary\", \"Multiple\",\n",
        "    ]] = Field(\n",
        "        default=None,\n",
        "        description=\"Describe the spatial distribution of masses.\"\n",
        "    )\n",
        "    modifiers: Optional[NoduleModifiers] = Field(\n",
        "        default=None,\n",
        "        description=\"Add location and chronicity details for the mass.\"\n",
        "    )\n",
        "\n",
        "class LungNodules(BaseModel):\n",
        "    \"\"\"\n",
        "    Groups subtypes of solid pulmonary lesions.\n",
        "    \"\"\"\n",
        "    nodule: Optional[Nodule] = Field(\n",
        "        default=None,\n",
        "        description=\"Provide findings related to pulmonary nodules (<3 cm).\"\n",
        "    )\n",
        "    mass: Optional[Mass] = Field(\n",
        "        default=None,\n",
        "        description=\"Provide findings related to pulmonary masses (>3 cm).\"\n",
        "    )"
      ],
      "metadata": {
        "id": "_Spm-sfbxYfZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# ROOT MODEL: STRUCTURED CXR FINDINGS\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "class CXRExtraction(BaseModel):\n",
        "    \"\"\"\n",
        "    Root model representing structured extraction of CXR findings.\n",
        "    \"\"\"\n",
        "    airspace_opacity: Optional[AirspaceOpacity] = Field(\n",
        "        default=None,\n",
        "        description=\"Include any airspace findings such as consolidation or ground-glass.\"\n",
        "    )\n",
        "    lung_nodules: Optional[LungNodules] = Field(\n",
        "        default=None,\n",
        "        description=\"Include any detected nodules or masses in the lungs.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "iBzCwX87xaLf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is time to test our complex response model."
      ],
      "metadata": {
        "id": "Kd4CvcFZxfHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampl_cxr_report = \"\"\"\n",
        "Clinical History: 65-year-old male with fever, cough, and recent weight loss.\n",
        "\n",
        "Findings:\n",
        "\n",
        "There is a solitary, well-circumscribed pulmonary nodule measuring approximately 8 mm in the left upper lobe. The nodule is partially calcified and appears non-suspicious on this study.\n",
        "\n",
        "There is a consolidative opacity involving the right lower lobe, consistent with lobar pneumonia. No cavitation or pleural effusion is identified. The consolidation demonstrates an acute pattern.\n",
        "\n",
        "No pulmonary mass is identified.\n",
        "\n",
        "Impression:\n",
        "\t•\tLeft upper lobe solitary pulmonary nodule, 8 mm, likely benign. Recommend follow-up imaging per Fleischner guidelines.\n",
        "\t•\tRight lower lobe consolidation, consistent with pneumonia.\n",
        "\t•\tNo evidence of pulmonary mass.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2lXja0N6rlHR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prompts\n",
        "test3_system_prompt = \"You are an expert in extracting data from radiology reports with 20 years of experience.\"\n",
        "test3_user_prompt = \"\"\"\n",
        "Extract data elements from the diagnostic radiology CXR report in the <report> tag:\n",
        "    Guidelines:\n",
        "    - Focus on findings at the time of scan, not previous ones.\n",
        "    - Ignore irrelevant information.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "qfYOJY37sWg5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's ask the model\n",
        "\n",
        "# Creating the conversation for the model to pass report and instructions\n",
        "messages3 = [\n",
        "        {\"role\": \"system\", \"content\": test3_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"{test3_user_prompt} \\n <report> {sampl_cxr_report} </report>\"}\n",
        "]\n",
        "\n",
        "# Asking the model to extract the requested information\n",
        "resp3 = patched_client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        response_model=CXRExtraction, # changing the repsponse model\n",
        "        messages=messages3,\n",
        "        temperature=TEMP,\n",
        "        seed=SEED,\n",
        "        max_retries=MAX_RETRIES,\n",
        ").model_dump_json(indent=4)\n",
        "\n",
        "\n",
        "rich.print('Output:', resp3)"
      ],
      "metadata": {
        "id": "H8Z_5ggQsppi",
        "outputId": "c8f1d480-1110-43dd-979c-fec49e66a43c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output: \u001b[1m{\u001b[0m\n",
              "    \u001b[32m\"airspace_opacity\"\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[32m\"consolidation\"\u001b[0m: \u001b[1m{\u001b[0m\n",
              "            \u001b[32m\"present\"\u001b[0m: true,\n",
              "            \u001b[32m\"extent\"\u001b[0m: \u001b[32m\"lobar\"\u001b[0m,\n",
              "            \u001b[32m\"modifiers\"\u001b[0m: \u001b[1m{\u001b[0m\n",
              "                \u001b[32m\"lung_lobe\"\u001b[0m: \u001b[32m\"Right Lower\"\u001b[0m,\n",
              "                \u001b[32m\"chronicity\"\u001b[0m: \u001b[32m\"Acute\"\u001b[0m\n",
              "            \u001b[1m}\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[32m\"ground_glass\"\u001b[0m: null\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[32m\"lung_nodules\"\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[32m\"nodule\"\u001b[0m: \u001b[1m{\u001b[0m\n",
              "            \u001b[32m\"distribution\"\u001b[0m: \u001b[32m\"Solitary\"\u001b[0m,\n",
              "            \u001b[32m\"modifiers\"\u001b[0m: \u001b[1m{\u001b[0m\n",
              "                \u001b[32m\"lung_lobe\"\u001b[0m: \u001b[32m\"Left Upper\"\u001b[0m,\n",
              "                \u001b[32m\"calcification\"\u001b[0m: \u001b[32m\"Punctate\"\u001b[0m,\n",
              "                \u001b[32m\"size_mm\"\u001b[0m: \u001b[1;36m8.0\u001b[0m,\n",
              "                \u001b[32m\"suspicious\"\u001b[0m: false\n",
              "            \u001b[1m}\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[32m\"mass\"\u001b[0m: null\n",
              "    \u001b[1m}\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Output: <span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">\"airspace_opacity\"</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">\"consolidation\"</span>: <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">\"present\"</span>: true,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">\"extent\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"lobar\"</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">\"modifiers\"</span>: <span style=\"font-weight: bold\">{</span>\n",
              "                <span style=\"color: #008000; text-decoration-color: #008000\">\"lung_lobe\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Right Lower\"</span>,\n",
              "                <span style=\"color: #008000; text-decoration-color: #008000\">\"chronicity\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Acute\"</span>\n",
              "            <span style=\"font-weight: bold\">}</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">\"ground_glass\"</span>: null\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">\"lung_nodules\"</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">\"nodule\"</span>: <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">\"distribution\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Solitary\"</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">\"modifiers\"</span>: <span style=\"font-weight: bold\">{</span>\n",
              "                <span style=\"color: #008000; text-decoration-color: #008000\">\"lung_lobe\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Left Upper\"</span>,\n",
              "                <span style=\"color: #008000; text-decoration-color: #008000\">\"calcification\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Punctate\"</span>,\n",
              "                <span style=\"color: #008000; text-decoration-color: #008000\">\"size_mm\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.0</span>,\n",
              "                <span style=\"color: #008000; text-decoration-color: #008000\">\"suspicious\"</span>: false\n",
              "            <span style=\"font-weight: bold\">}</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">\"mass\"</span>: null\n",
              "    <span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "This notebook demonstrated how to achieve more consistent and structured output from Large Language Models (LLMs) by leveraging the `instructor` and `pydantic` libraries within a Google Colab environment. We first illustrated the potential inconsistencies of standard LLM outputs when asked to extract structured data. Then, we showed how patching the OpenAI client with `instructor` and defining a `pydantic` `BaseModel` allows for validation and automatic retries, ensuring the LLM's response adheres to a specified schema. Finally, we explored how to handle more complex data extraction scenarios, such as analyzing a chest X-ray report, by using nested `pydantic` models to create a hierarchical representation of the findings. This approach significantly enhances the reliability and usability of LLM-generated data for downstream tasks."
      ],
      "metadata": {
        "id": "Ew3k8o28xqJK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PotrUQLWmrtA"
      },
      "source": [
        "**Authors**:\n",
        "- [Ali Ganjizadeh, M.D](https://www.linkedin.com/in/magnooj)\n",
        "- [Bradley J. Erickson, M.D., Ph.D.](https://www.linkedin.com/in/bradleyerickson/)\n",
        "\n",
        "This notebook is a part of [MIDel.org](http://midel.org/). `MIDeL` is a website to help healthcare professionals and medical imaging scientists learn to apply deep learning methods to medical images."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}