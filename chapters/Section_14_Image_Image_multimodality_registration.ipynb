{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "Section 14: Image-Image multimodality registration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mayo-Radiology-Informatics-Lab/MIDeL-Chapters/blob/main/chapters/Section_14_Image_Image_multimodality_registration.ipynb)"
      ],
      "metadata": {
        "id": "ghJw2xL8ZPCY"
      },
      "id": "ghJw2xL8ZPCY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Author: Shahriar Faghani, M.D.*"
      ],
      "metadata": {
        "id": "C1eFGlEUZLo1"
      },
      "id": "C1eFGlEUZLo1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14. Multimodal Deep Learning Models\n",
        "\n"
      ],
      "metadata": {
        "id": "6QgJ7B97hq9J"
      },
      "id": "6QgJ7B97hq9J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Similar to human physicians, automated detection and classification systems that use both medical imaging data and clinical data from the EHR -- such as patient demographics, previous diagnoses, and laboratory data -- are likely to produce models with better performance. Recent medical imaging literature also shows a similar trend where both EHR and pixel data are exploited in a \"fusion paradigm\" for solving complex tasks that cannot easily be solved by one modality alone (Huang et al. 2020)."
      ],
      "metadata": {
        "id": "ZkgVnPFohzbt"
      },
      "id": "ZkgVnPFohzbt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###14-A imaging - imaging models:"
      ],
      "metadata": {
        "id": "4rmjTMpiitfW"
      },
      "id": "4rmjTMpiitfW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Contemporary medicine relies on the synthesis of a wide variety of information such as imaging data, laboratory data, unstructured narrative data, 1D signals like EKG, and in some cases, audio or observational data. The clinical context of images often plays a critical role in providing diagnosis decisions in medical image interpretation. For example, it has repeatedly been shown that a lack of access to clinical and laboratory data during image interpretation results in lower performance and decreased clinical utility for the referring provider. In a survey of radiologists, the majority (87%) stated that clinical information had a significant impact on interpretation (Boonn and Langlotz 2009). Radiology is not the only imaging-based medical specialty that relies on context for accurate interpretation of imaging data; pathology, ophthalmology, and dermatology also use clinical context for clinical image interpretation. With accurate and relevant information regarding current symptoms and past medical history, physicians are better able to interpret imaging findings in the appropriate clinical context, resulting in a more relevant differential diagnosis, a more useful report for physicians, and a more favorable outcome for the patient. \n",
        "####Convolutional neural network (CNN) models typically use pixel values as inputs, without contextualizing other clinical information as doctors do in clinical practice, which may limit performance. As an example consider the “simple” task in radiology of identifying pneumonia on a chest x-ray (CXR), something that has been achieved by many investigators training deep learning models for automated detection and classification of pathologies. Such applications, however, may ultimately have a limited impact on clinical practice without clinical context such as chief complaint, history of present illness, past medical history, drug history, family history, physical examination findings, and laboratory values. In spite of having imaging findings that can set pneumonia apart from other diagnoses, CXR is nonspecific, and accurate diagnosis requires clinical and laboratory information. CXR findings that suggest pneumonia would be accurate in patients with fevers and elevated white blood cell counts, but in patients with no similar clinical features or laboratory values, similar images might indicate atelectasis, pulmonary edema, interstitial disease, or even lung cancer. \n",
        "\n",
        "####First, we will discuss different fusion techniques in deep learning and machine learning, and then we will discuss some medical multimodality examples, and finally, we will develop a simple multimodal model.\n"
      ],
      "metadata": {
        "id": "moQxFUkHixWb"
      },
      "id": "moQxFUkHixWb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fusion Techniques"
      ],
      "metadata": {
        "id": "dd8R3-9ui87u"
      },
      "id": "dd8R3-9ui87u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Image fusion is a technique for combining the information from several imaging data sources acquired from the same or different modalities. A goal of image fusion, especially in medical imaging, is to enhance or complement each data source's features, so that machine learning algorithms can achieve better performance rather than using only one data channel. The expected output will have a richer feature representation than the individual components. The multimodal fusion technique has achieved remarkable success in a variety of applications like medical image segmentation, image classification, and image reconstruction.\n",
        "####Data fusion requires more than one data source (e.g. could be from one exam like T1 and T2 of MRI or different exams like MRI and CT). The traditional image processing literature refers to these as ‘channels’ and we will also use that nomenclature. Data channels can be heterogeneous, complementary, concordant or discordant, synchronous or asynchronous, and redundant with different scales that must be normalized in order to be combined. Medical images such as MRI/CT/PET/Ultrasound also have differences that must be considered when using them in combination. Therefore, the choice of algorithm or architecture depends primarily on the goal of the application and data channel characteristics. To summarize, the advantages of data fusion (Bellot et al. 2002) are:\n",
        "####1) To provide different types of information to help increase certainty about the diagnosis.\n",
        "####2) To combine source features and extract the new or hidden information with increased certainty.\n",
        "####3) To enhance the abstraction and completeness of data by efficiently combining them.\n",
        "####In recent computer vision publications, people investigate the multimodal fusion strategies in the context of the spatio-temporal convolutional neural network (Karpathy et al. 2014) and broadly categorized into four fusion patterns/classes: Early fusion, Late fusion, Joint fusion, and Slow fusion.    \t\n"
      ],
      "metadata": {
        "id": "u2As3ZN2jAQ4"
      },
      "id": "u2As3ZN2jAQ4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Early Fusion:"
      ],
      "metadata": {
        "id": "aWKbcfE2jJpo"
      },
      "id": "aWKbcfE2jJpo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Early fusion merges the various data channels at the beginning of the machine learning model training. These individual modality features may be joined in different ways, such as by average pooling, weighted concatenation, or gated fusion techniques (Type I or II in Fig. 1). The unimodal local features are initially extracted from their individual inputs and then concatenated into a joint representation. Therefore, the unified model must ensure that the data/features are correctly aligned in order to enable joint processing. As a result, converting data sources into a single feature vector is a significant challenge in early data fusion approaches. In reality, the conditionally independent criteria between medical data sources are not always true as multiple modalities may have highly correlated features, possibly because of acquisition setting. Therefore, different data collection modalities are typically spatially aligned (known as image registration) with a standard pixel sampling rate (for images or perhaps time for other modalities) to make a common ground for fusion. If the data is aligned correctly, the cross-correlations between data items may be exploited, which provides an opportunity to increase the system performance (Gadzicki et al. 2020).\n"
      ],
      "metadata": {
        "id": "6zOul7QujN5j"
      },
      "id": "6zOul7QujN5j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.ibb.co/WchcPN1/fig1.png\"><br>\n",
        "*Figure 1.* **Early Fusion**<br><br>\n"
      ],
      "metadata": {
        "id": "CSEJLXi-jRuH"
      },
      "id": "CSEJLXi-jRuH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Late or Ensemble Fusion:\n",
        "####In late fusion, the individual streams fully process each data channel separately and their outputs are merged at a decision-making or prediction stage through a summation, averaging operation, or majority voting. Similar to the ensemble technique, the late fusion process learns the optimal combination of each component that maximizes performance. The learning process handles the errors independently, decorrelates errors, and thus improves performance with respect to each modality. The major drawback lies in the limited potential for the exploitation of cross-correlations between the different unimodal data.\n",
        "<img src=\"https://i.ibb.co/T2JHW4X/fig2.png\"><br>\n",
        "*Figure 2.* **Late Fusion** <br><br>\n",
        "###Joint Fusion\n",
        "####Joint fusion was introduced to fuse the imaging feature representations with the clinical features before feeding them into the model. Because of the differences between the imaging and clinical features in dimensionality and dynamic range, different models were introduced (Haylat 2020) to scale their clinical features before fusion and improve their performance (Fig. 3).\n",
        "<img src=\"https://i.ibb.co/kKxVYfz/fig3.png\"><br>\n",
        "*Figure 3.* **Joint Fusion, Type I (Up) and Type II (down)** <br><br>\n",
        "###Slow Fusion\n",
        "####Slow fusion is widely used in 3D video both the spatial and temporal dimensions (Huang et al. 2020) begin as multiple networks that take consecutive video frames and slowly fuse their temporal features throughout the network such that the higher layers progressively get access to global information(Karpathy et al. 2014). As shown in Fig. 4, the whole architecture is segregated into four layers of actions that progressively share the training parameters. In the first layer, each model takes four temporal sequences (stride of four) that pass through a series of convolutions, producing four feature maps. The second layer processes a series of two temporal features out of four feature maps and passes it to layer three, consequently accessing information across all the input video frames. Finally, layer four performs a pooling operation for global feature extraction and eventually connects higher tasks (classification) with a dense connection. Therefore, the machine learning or CNN models benefit from learning the powerful features with subsequent fusion connections and can be robust to details of the connectivity across time.\n",
        "<img src=\"https://i.ibb.co/St6Ccz9/fig4.png\"><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "voChWu2OoCdj"
      },
      "id": "voChWu2OoCdj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Imaging Only Multimodal Learning:\n",
        "\n",
        "####As we discussed before, multimodality can be derived from a variety of sources; this section discusses image-image learning, while sections 14-B and 14-C consider imaging-tabular data learning, and imaging-text learning, respectively.\n",
        "\n",
        "####Multimodal imaging studies usually require co-registration. The process of co-registering images includes geometrically aligning two or more images so that their corresponding pixels (voxels) represent the same object. In general, image co-registration can be classified into two categories based on the reference: one approach registers the images into an existing atlas; the other approach registers the images into one selected image. It is essential that this process is undertaken before any subsequent quantitative image analysis. Various techniques are available for doing image registration, including DL-based registration and traditional registration, which are beyond the scope of this chapter (for further reading see (Haskins, Kruger, and Yan 2020)). \n",
        "####Multimodal imaging has many real-world examples, one of which is the fusion of structural and functional (molecular) imaging such as PET-CT and PET-MR. Since molecular imaging has a limited spatial resolution, it cannot precisely show the location of tumors, and tracers often are not taken up by enough structures to make anatomic location clear. By acquiring CT or MR images within the same physical device, the precise anatomic location can be determined(Fig. 5).\n",
        "<img src=\"https://i.ibb.co/vZf4V1b/fig5.png\"><br>\n",
        "*Figure 5.* **(a) Coregistered PET-CT scan , (b) CT scan,(c) PET scan**<br><br>\n",
        "####CT-MRI fusion for radiation therapy is another example. MRI has superb soft-tissue resolution and can show tumor which is difficult to appreciate on CT. On the other hand, MRI does not reflect radiation attenuation, which is critical to accurate dose delivery. As a result,fusion of MRI images with CT is commonly used in radiosurgery, interventional radiology, and radiotherapy. (Liu et al. 2019) (Fig. 6)\n",
        "<img src=\"https://i.ibb.co/qjyf2Z6/fig6.png\"><br>\n",
        "*Figure 6.* **(a) CT scan, (b) Coregistered CT-MRI scan,(c) MRI scan\t**<br><br>\n",
        "####While we used different modalities in the above examples, there are also times when planes or modes of acquisition of the same modality can yield different types of information that can be utilized. For instance, using both the mediolateral oblique (MLO) and the craniocaudal (CC) mammogram views increases the accuracy of diagnostic imaging for detecting breast cancers, compared to only using one. (Fig. 7)\n"
      ],
      "metadata": {
        "id": "kizWyMnd1GGn"
      },
      "id": "kizWyMnd1GGn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS5x_SCTo0jQ"
      },
      "source": [
        "#### In this notebook we are going to train a model to determine MGMT promoter methylation status within patients with GBM tumor.\n",
        "##### We are going to use T1,T2,T1 post contrast,FLAIR MRI sequences and also mask of tumor."
      ],
      "id": "jS5x_SCTo0jQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5e00222-12f4-4db8-8293-823461a3b0f6"
      },
      "source": [
        "### Import Packages"
      ],
      "id": "c5e00222-12f4-4db8-8293-823461a3b0f6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1Sz4Q0MeDPm",
        "outputId": "8f9c3f40-6576-4127-df2c-2743c111baf1"
      },
      "source": [
        "!pip install monai wandb -q"
      ],
      "id": "X1Sz4Q0MeDPm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 721 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 53.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 70.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 72.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c74533f5"
      },
      "source": [
        "import os\n",
        "\n",
        "import math\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import monai as mn\n",
        "import nibabel as nib\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader,WeightedRandomSampler\n",
        "import wandb"
      ],
      "id": "c74533f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0613f8d5-7d5b-48d0-acd6-8ac9e59a0d9a"
      },
      "source": [
        "### Select one GPU if there are many / Set up WANDB environment"
      ],
      "id": "0613f8d5-7d5b-48d0-acd6-8ac9e59a0d9a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "337fa6ed"
      },
      "source": [
        "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
        "os.environ['WANDB_API_KEY']=#Enter your WAND API KEY here.\n",
        "os.environ['WANDB_SILENT']='true'"
      ],
      "id": "337fa6ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b67fc951-d4fa-4a2b-a036-bc8ee974652a"
      },
      "source": [
        "### Make Training Proccess Deterministic"
      ],
      "id": "b67fc951-d4fa-4a2b-a036-bc8ee974652a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb668ba2"
      },
      "source": [
        "def seed_all(seed:int) -> None:\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    mn.utils.misc.set_determinism(seed=seed)\n",
        "seed_all(123)"
      ],
      "id": "cb668ba2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71a7990d-8a26-40d8-9441-11a121b400e2"
      },
      "source": [
        "### Set up some Hyperparameters"
      ],
      "id": "71a7990d-8a26-40d8-9441-11a121b400e2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93c68dd2"
      },
      "source": [
        "#Hyperparameter\n",
        "bs=4\n",
        "lr=1e-5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "epochs=15"
      ],
      "id": "93c68dd2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6483545-889e-457e-82d5-c6d84a66f98a"
      },
      "source": [
        "#### Create a fucntion for visualization"
      ],
      "id": "e6483545-889e-457e-82d5-c6d84a66f98a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ad64ad4"
      },
      "source": [
        "def show_slices(slices):\n",
        "    fig, axes = plt.subplots(1, len(slices))\n",
        "    for i, slice in enumerate(slices):\n",
        "        axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")"
      ],
      "id": "7ad64ad4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb3dd95e-cb80-491e-8ecd-198504abab34"
      },
      "source": [
        "### Load Labels(MGMT methylation status) for each patient"
      ],
      "id": "bb3dd95e-cb80-491e-8ecd-198504abab34"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To download the dataset click on the links below and make a shortcut of them in your google drive.\n",
        "Link to the NIfTI files:https://drive.google.com/drive/folders/1-vASO92VQ-KLSsoBakgySID0fGbdBRQb?usp=sharing \n",
        "\n",
        "Link to the CSV file:https://drive.google.com/file/d/1-HqrqZKPrJfw0GqrcD55_x-C3BrffTPW/view?usp=sharing\n"
      ],
      "metadata": {
        "id": "ntPI5qa5lSeG"
      },
      "id": "ntPI5qa5lSeG"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PABp4bN9jpIB",
        "outputId": "c3e36958-4a5f-4160-f3b5-2ff28a2c52b8"
      },
      "id": "PABp4bN9jpIB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f855e6f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "9a8570bd-7d06-45a1-b7aa-a00a702f4246"
      },
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/MGMT/train_labels.csv')# csv path\n",
        "df.head(3)"
      ],
      "id": "f855e6f4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   BraTS21ID  MGMT_value\n",
              "0          0           1\n",
              "1          2           1\n",
              "2          3           0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-440bed11-ee14-452a-8f99-878cec3322d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BraTS21ID</th>\n",
              "      <th>MGMT_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-440bed11-ee14-452a-8f99-878cec3322d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-440bed11-ee14-452a-8f99-878cec3322d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-440bed11-ee14-452a-8f99-878cec3322d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98b94e70-5617-4930-ae80-24aac4f8f7c3"
      },
      "source": [
        "### Splitting dataset to train and validation sets at patients level"
      ],
      "id": "98b94e70-5617-4930-ae80-24aac4f8f7c3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c9f0b69"
      },
      "source": [
        "df_train,df_val=train_test_split(df, test_size=0.19, train_size=0.81, random_state=123 ,shuffle=True, stratify=df['MGMT_value'])"
      ],
      "id": "1c9f0b69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "364322f2",
        "outputId": "22b39eaa-b89f-4ac4-fd4c-16314681a96d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Number of positive train samples:',len(df_train[df_train['MGMT_value']==1]))\n",
        "print('Number of negative train samples:',len(df_train[df_train['MGMT_value']==0]))\n",
        "print('Number of positive validation samples:',len(df_val[df_val['MGMT_value']==1]))\n",
        "print('Number of negative validation samples:',len(df_val[df_val['MGMT_value']==0]))\n",
        "print('Ratio of positive to negative samples in train set:',len(df_train[df_train['MGMT_value']==1])/len(df_train[df_train['MGMT_value']==0]))\n",
        "print('Ratio of positive to negative samples in validation set:',len(df_val[df_val['MGMT_value']==1])/len(df_val[df_val['MGMT_value']==0]))"
      ],
      "id": "364322f2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of positive train samples: 248\n",
            "Number of negative train samples: 225\n",
            "Number of positive validation samples: 59\n",
            "Number of negative validation samples: 53\n",
            "Ratio of positive to negative samples in train set: 1.1022222222222222\n",
            "Ratio of positive to negative samples in valodation set: 1.1132075471698113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d760240b",
        "outputId": "6d597a37-0af3-4e28-f9be-6c758045f4b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "df['MGMT_value'].hist(grid=False,bins=3)\n",
        "len_neg=len(df[df['MGMT_value']==0])\n",
        "len_pos=len(df[df['MGMT_value']==1])\n",
        "print(f'Number of Total positive samples:{len_pos}\\nNumber of Total Negative samples:{len_neg}')"
      ],
      "id": "d760240b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Total positive samples:307\n",
            "Number of Total Negative samples:278\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfdb113f-cff3-48e2-80c5-22edefef8daf"
      },
      "source": [
        "### Retrieve image paths"
      ],
      "id": "bfdb113f-cff3-48e2-80c5-22edefef8daf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00dabc5e"
      },
      "source": [
        "list_0=os.listdir('/content/drive/MyDrive/MGMT/RSNA_ASNR_MICCAI_BraTS2021_TrainingData')"
      ],
      "id": "00dabc5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99898295"
      },
      "source": [
        "def path_maker_list(patient_list:list,path:str):\n",
        "    path_list=[]\n",
        "    for item in patient_list:\n",
        "        path_patient=os.path.join(path,item)\n",
        "        path_list.append(path_patient)\n",
        "    return path_list"
      ],
      "id": "99898295",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47be0a64",
        "outputId": "c6c25b0c-1402-49f1-a364-d96325ebe135",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "path_list_0=path_maker_list(list_0,'/content/drive/MyDrive/MGMT/RSNA_ASNR_MICCAI_BraTS2021_TrainingData')\n",
        "len(path_list_0)"
      ],
      "id": "47be0a64",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1250"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_list_0[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aHTFep0hbhM5",
        "outputId": "87cdd087-68ef-488d-d6df-cea3e5522f26"
      },
      "id": "aHTFep0hbhM5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/MGMT/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01074'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fea28059-9937-4583-ab1c-eb6ae48d7704"
      },
      "source": [
        "##### Create List of Dictionaries that for each patient contain: Paths of different MRI sequences and Label"
      ],
      "id": "fea28059-9937-4583-ab1c-eb6ae48d7704"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84c06fd3",
        "outputId": "c4e6437a-f20b-4ec8-9e5c-15caa2a7fc59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_list=[]\n",
        "val_list=[]\n",
        "for i,idx in enumerate(path_list_0):\n",
        "    data_dict={}\n",
        "    id_num=int(idx.split('/')[-1].split('_')[1])\n",
        "    df_selected=df.loc[lambda df: df['BraTS21ID'] == id_num]\n",
        "    idx_str=idx.split('/')[-1].split('_')[1]\n",
        "    try:\n",
        "      label=list(df_selected['MGMT_value'])[0]\n",
        "      data_dict['img1']=f'{idx}/BraTS2021_{idx_str}_t1.nii.gz'\n",
        "      data_dict['img2']=f'{idx}/BraTS2021_{idx_str}_t2.nii.gz'\n",
        "      data_dict['img1c']=f'{idx}/BraTS2021_{idx_str}_t1ce.nii.gz'\n",
        "      data_dict['imgf']=f'{idx}/BraTS2021_{idx_str}_flair.nii.gz'\n",
        "      data_dict['seg']=f'{idx}/BraTS2021_{idx_str}_seg.nii.gz'\n",
        "      data_dict['label']=label\n",
        "    except:\n",
        "      pass\n",
        "    if id_num in list(df_train['BraTS21ID']):# and os.path.isfile(data_dict['img1']) and  os.path.isfile(data_dict['img2']) and os.path.isfile(data_dict['img1c']) and os.path.isfile(data_dict['imgf']):\n",
        "        train_list.append(data_dict)\n",
        "    elif id_num in list(df_val['BraTS21ID']):\n",
        "        val_list.append(data_dict)\n",
        "print(f'Number of train samples:{len(train_list)}')\n",
        "print(f'Number of validation samples:{len(val_list)}')"
      ],
      "id": "84c06fd3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train samples:467\n",
            "Number of validation samples:109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af59ae9a-939e-4df3-a26f-a62740b5eafe"
      },
      "source": [
        "### Transformations"
      ],
      "id": "af59ae9a-939e-4df3-a26f-a62740b5eafe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "572ce3c3"
      },
      "source": [
        "####In the current notebook we just use post contast T1 and the segmentation,but you can use other sequnces as well by modifying 'keys' argument in transformations\n",
        "train_transforms=mn.transforms.Compose([\n",
        "    mn.transforms.LoadImageD(keys=[\"img1c\",'seg']),\n",
        "    mn.transforms.AddChannelD(keys=[\"img1c\",'seg']),\n",
        "    mn.transforms.SpacingD(keys=[\"img1c\",'seg'],pixdim=(1,1,1), meta_key_postfix='meta_dict'),\n",
        "    mn.transforms.Resized(keys=[\"img1c\",'seg'], spatial_size=(240,240,155)),\n",
        "    mn.transforms.NormalizeIntensityD(keys=[\"img1c\"]),\n",
        "    mn.transforms.ConcatItemsd(keys=[\"img1c\",'seg'], name=\"img\"),\n",
        "    mn.transforms.RandAffineD(\n",
        "        keys=\"img\",\n",
        "        translate_range=(15,15,10),\n",
        "        scale_range=(0.05,0.05,0.05),\n",
        "        rotate_range=(math.pi/12,math.pi/12,math.pi/15),\n",
        "        padding_mode='zeros',\n",
        "        prob=0.5,\n",
        "        as_tensor_output=False),\n",
        "    mn.transforms.ToTensord(keys=[\"img\",\"label\"]),\n",
        "    \n",
        "])\n",
        "val_transforms=mn.transforms.Compose([\n",
        "    mn.transforms.LoadImageD(keys=[\"img1c\",'seg']),\n",
        "    mn.transforms.AddChannelD(keys=[\"img1c\",'seg']),\n",
        "    mn.transforms.SpacingD(keys=[\"img1c\",'seg'],pixdim=(1,1,1), meta_key_postfix='meta_dict'),\n",
        "    mn.transforms.Resized(keys=[\"img1c\",'seg'], spatial_size=(240,240,155)),\n",
        "    mn.transforms.NormalizeIntensityD(keys=[\"img1c\"]),\n",
        "    mn.transforms.ConcatItemsd(keys=[\"img1c\",'seg'], name=\"img\"),\n",
        "    mn.transforms.NormalizeIntensityD(keys=\"img\"),\n",
        "    mn.transforms.ToTensord(keys=[\"img\",\"label\"]),\n",
        "    \n",
        "])"
      ],
      "id": "572ce3c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "040b095e-e71c-4fa6-8f1c-1eb26410bc91"
      },
      "source": [
        "### Create Dataloader/Dataset/Sampler"
      ],
      "id": "040b095e-e71c-4fa6-8f1c-1eb26410bc91"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c36a2ae6"
      },
      "source": [
        "\"\"\"How make batch in dataloader\"\"\"\n",
        "def collater(data):\n",
        "    batch_imgs = [s['img'] for s in data]\n",
        "    batch_labels = [s['label'] for s in data]\n",
        "    imgs = torch.stack(batch_imgs, axis=0)\n",
        "    targets = torch.stack(batch_labels, axis=0)\n",
        "    targets=targets.squeeze(-1)\n",
        "    return {'imgs': imgs, 'targets': targets}"
      ],
      "id": "c36a2ae6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff21f92d"
      },
      "source": [
        "'''Creating Dataset'''\n",
        "train_ds = mn.data.Dataset(data=train_list, transform=train_transforms)\n",
        "val_ds = mn.data.Dataset(data=val_list, transform=val_transforms)"
      ],
      "id": "ff21f92d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1208268f",
        "outputId": "08d88b1c-8507-4b1c-c1c9-e3295b09c886",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for data in train_ds[:2]:\n",
        "    print(data['img'].shape)"
      ],
      "id": "1208268f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 240, 240, 155])\n",
            "torch.Size([2, 240, 240, 155])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a252b489"
      },
      "source": [
        "'''Sampler'''\n",
        "class_weights=[1.1,1]\n",
        "sample_weights=[0]*len(train_ds)\n",
        "for idx,data in enumerate(train_ds):\n",
        "    class_weight=class_weights[data['label']]\n",
        "    sample_weights[idx]=class_weight\n",
        "sampler= WeightedRandomSampler(sample_weights,num_samples=len(sample_weights),replacement=True)"
      ],
      "id": "a252b489",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68522725",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "203892f6-9b27-4e7c-eb1a-5ea987d7822b"
      },
      "source": [
        "\"\"\"Creating train and validation dataloader\"\"\"\n",
        "train_loader = DataLoader(train_ds, batch_size=bs, num_workers=4, pin_memory=torch.cuda.is_available(),collate_fn=collater,sampler=sampler,prefetch_factor=1)\n",
        "val_loader = DataLoader(val_ds, batch_size=bs, num_workers=4, pin_memory=torch.cuda.is_available(),collate_fn=collater,prefetch_factor=1)"
      ],
      "id": "68522725",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4b10926-9dd7-469f-a777-a37febf9077a"
      },
      "source": [
        "### Create model"
      ],
      "id": "c4b10926-9dd7-469f-a777-a37febf9077a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dee9b8f1"
      },
      "source": [
        "model = mn.networks.nets.DenseNet121(spatial_dims=3, in_channels=2, out_channels=2).to(device)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)"
      ],
      "id": "dee9b8f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "943bc88b-1e8f-4bb8-9b4e-3454aab4f8e2"
      },
      "source": [
        "### Training and evaluation on Validation set\n",
        "\n"
      ],
      "id": "943bc88b-1e8f-4bb8-9b4e-3454aab4f8e2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e98ac062"
      },
      "source": [
        "val_interval = 1\n",
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "wandb.init(project='midel_test', entity='shahriar-faghani')\n",
        "config = wandb.config\n",
        "config.learning_rate = lr\n",
        "config.batch_size=bs\n",
        "config.mode='3D'\n",
        "config.backbone='DenseNet121'\n",
        "config.optimizer='Adam'\n",
        "config.sampler='True'\n",
        "config.resample='True'\n",
        "config.registration='True'\n",
        "config.normalization='Per patient'\n",
        "config.epochs=epochs\n",
        "config.augmentation='Affine'\n",
        "wandb.run.name=f'BraTS_3D_Densenet121_{lr}_bs{bs}_2Channel(1c_seg)_normalization_pp'\n",
        "wandb.watch(model)\n",
        "\n",
        "for i,epoch in enumerate(tqdm(range(epochs))):\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"epoch {epoch + 1}/{epochs}\")\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    val_epoch_loss=0\n",
        "    step = 0\n",
        "    val_step=0\n",
        "    for batch_data in tqdm(train_loader):\n",
        "        step += 1\n",
        "        inputs, labels = batch_data[\"imgs\"].to(device), batch_data[\"targets\"].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_len = (len(train_ds) // train_loader.batch_size)+1\n",
        "        print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
        "        wandb.log({\"loss\": loss})\n",
        "    epoch_loss /= step\n",
        "    wandb.log({'epoch_loss':epoch_loss})\n",
        "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
        "        y = torch.tensor([], dtype=torch.long, device=device)\n",
        "        for val_data in val_loader:\n",
        "            val_step +=1\n",
        "            val_images, val_labels = val_data[\"imgs\"].to(device), val_data[\"targets\"].to(device)\n",
        "            val_output=model(val_images)\n",
        "            val_loss= loss_function(val_output,val_labels)\n",
        "            val_epoch_loss += val_loss.item()\n",
        "            val_epoch_len = len(val_ds) // val_loader.batch_size\n",
        "            total_val_step=(len(val_ds)//bs)+1\n",
        "            print(f\"{val_step}/{total_val_step}, val_loss: {val_loss.item():.4f}\")\n",
        "            wandb.log({'val_loss':val_loss})\n",
        "            y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n",
        "            y = torch.cat([y, val_labels], dim=0)\n",
        "            \n",
        "\n",
        "        val_epoch_loss /= val_step\n",
        "        print(f\"epoch {epoch + 1} average val_loss: {val_epoch_loss:.4f}\")\n",
        "        wandb.log({'val_epoch_loss':val_epoch_loss})\n",
        "\n",
        "        acc_value_val = torch.eq(y_pred.argmax(dim=1), y)\n",
        "        acc_metric_val = acc_value_val.sum().item() / len(acc_value_val)\n",
        "        if acc_metric_val > best_metric:\n",
        "            best_metric = acc_metric_val\n",
        "            best_metric_epoch = epoch + 1\n",
        "            ##torch.save(model.state_dict(), \"best_metric_model_classification3d_aux_loss_dict.pth\") Specify Directory\n",
        "            print(\"saved new best metric model\")\n",
        "        print(f\"current epoch: {epoch + 1} current accuracy: {acc_metric_val:.4f} best accuracy: { best_metric:.4f}\")\n",
        "        wandb.log({\"loss\": loss,\n",
        "                    'Accuracy_val':acc_metric_val,})\n",
        "            #writer.add_scalar(\"val_accuracy\", acc_metric, epoch + 1)\n",
        "        y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
        "        y = torch.tensor([], dtype=torch.long, device=device)\n",
        "        \n",
        "        step = 0\n",
        "        train_step=0\n",
        "        for train_data in train_loader:\n",
        "            train_step +=1\n",
        "            train_images, train_labels = train_data[\"imgs\"].to(device), train_data[\"targets\"].to(device)\n",
        "            train_output=model(train_images)\n",
        "            train_epoch_len = len(train_ds) // train_loader.batch_size\n",
        "            total_train_step=(len(train_ds)//bs)+1\n",
        "            y_pred = torch.cat([y_pred, model(train_images)], dim=0)\n",
        "            y = torch.cat([y, train_labels], dim=0)\n",
        "            \n",
        "\n",
        "        acc_value_train = torch.eq(y_pred.argmax(dim=1), y)\n",
        "        acc_metric_train = acc_value_train.sum().item() / len(acc_value_train)\n",
        "        wandb.log({'Accuracy_train':acc_metric_train})\n",
        "\n",
        "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")"
      ],
      "id": "e98ac062",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###References:\n",
        "1. Bellot, D., Boyer, A. and Charpillet, F. (2002) ‘A new definition of qualified gain in a data fusion process: application to telemedicine’, in Proceedings of the Fifth International Conference on Information Fusion. FUSION 2002. (IEEE Cat.No.02EX5997), pp. 865–872 vol.2.\n",
        "2. Gadzicki, K., Khamsehashari, R. and Zetzsche, C. (2020) ‘Early vs Late Fusion in Multimodal Convolutional Neural Networks’, in 2020 IEEE 23rd International Conference on Information Fusion (FUSION), pp. 1–6.\n",
        "3. Karpathy, A. et al. (2014) ‘Large-scale video classification with convolutional neural networks’, in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 1725–1732.\n",
        "4. Huang, S.-C. et al. (2020) ‘Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines’, npj Digital Medicine. doi:10.1038/s41746-020-00341-z.\n",
        "5. Haileleol Tibebu (2020), Introduction To Data Fusion, URL: https://medium.com/haileleol-tibebu/data-fusion-78e68e65b2d1\n",
        "6. Yoo, Y. et al. (2019) ‘Deep learning of brain lesion patterns and user-defined clinical and MRI features for predicting conversion to multiple sclerosis from clinically isolated syndrome’, Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization, 7(3), pp. 250–259.iomed. Eng. Imaging Vis. 7, 250–259, (2019).\n",
        "7. Boonn, William W., and Curtis P. Langlotz. 2009. “Radiologist Use of and Perceived Need for Patient Data Access.” Journal of Digital Imaging 22 (4): 357–62.\n",
        "8. Haskins, Grant, Uwe Kruger, and Pingkun Yan. 2020. “Deep Learning in Medical Image Registration: A Survey.” Machine Vision and Applications 31 (1): 8.\n",
        "9. Huang, Shih-Cheng, Anuj Pareek, Saeed Seyyedi, Imon Banerjee, and Matthew P. Lungren. 2020. “Fusion of Medical Imaging and Electronic Health Records Using Deep Learning: A Systematic Review and Implementation Guidelines.” NPJ Digital Medicine 3 (October): 136.\n",
        "10. Liu, Shi-Feng, Jian Lu, Hong Wang, Yan Han, De-Feng Wang, Li-Li Yang, Zi-Xiang Li, and Xiao-Kun Hu. 2019. “Computed Tomography-Magnetic Resonance Imaging Fusion-Guided Iodine-125 Seed Implantation for Single Malignant Brain Tumor: Feasibility and Safety.” Journal of Cancer Research and Therapeutics 15 (4): 818–24.\n"
      ],
      "metadata": {
        "id": "oELTViNW2EIg"
      },
      "id": "oELTViNW2EIg"
    }
  ]
}